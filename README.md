# Image-to-text-translator

This repository contains a Jupyter Notebook that demonstrates image captioning using the BLIP (Bootstrapped Language-Image Pretraining) model. The project leverages pre-trained BLIP models to generate descriptive captions for input images, making it useful for tasks like accessibility, automated tagging, and content understanding.

Features
1)Loads and processes images for captioning
2)Utilizes BLIP for generating text descriptions
3)Runs in Google Colab for easy execution

Requirements
1)Python 3.x
2)Torch, Transformers (Hugging Face)
3)Jupyter Notebook / Google Colab
